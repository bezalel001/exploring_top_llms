{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708773d3",
   "metadata": {},
   "source": [
    "# Safety and Security\n",
    "When working with LLMs, you must consider safety and security before you deploy the system to production. An LLM should be protected from harmful user prompts(security), and users should be protected from harmful LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb5bfb",
   "metadata": {},
   "source": [
    "## Security\n",
    " - preventing losses due to intentional actions by malevolent actors\n",
    " - LLM security ensures that only trusthworthy prompts are processed\n",
    " - An LLM should be protected from threats and vulnerabilities that could compromise their integrity, confidentiality or availability\n",
    "\n",
    "### Hackers\n",
    "Malicious actors might attempt to compromise LLMs through various methods, like adversarial attacks to mislead the model or prompt injections to extract sensitive information\n",
    "\n",
    "### Data poisoning\n",
    "In a data poisoning threat, attackers corrupt the training dat to embed biases\n",
    "\n",
    "### Robustness\n",
    "Robustness ensures that any attempt to exploit weaknesses in the model for malicious purposes is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8415f",
   "metadata": {},
   "source": [
    "## Safety\n",
    "- Preventing losses due to unintentional actions by benevolent actors\n",
    "- LLM safety corresponds to strategies that are implemented to ensure that an LLM operates in a responsible and ethical way and that it does not provide harmful content.\n",
    "- Safety focuses on the mitigation of potential risks to its users or to other systems that interact with the LLM\n",
    "\n",
    "### Biases\n",
    "Harmful biases in training dat should be avoided\n",
    "\n",
    "### Hallucinations\n",
    "LLM confidently generating incorrect or misleading information should be mitigated\n",
    "\n",
    "### Personally Identifiable information PII\n",
    "A model should not leak any PII\n",
    "\n",
    "### Ethics\n",
    "Ensure that LLMs align with ehtical guidelines and void generating harmful recommendations\n",
    "\n",
    "### Of-topic\n",
    "A chattbot typically is centred around a specific topic and should only answer questions on that topic. Safety measures can help keep a model focused on that topic\n",
    "\n",
    "### Reputational Damage\n",
    "A chatbot that provides content that is not well aligned with the topic or with company values can easily damage the reputation of the company\n",
    "\n",
    "### Robustness\n",
    "The LLM should be protected from being exploited or manipulated to extract secrets or generate malicious content\n",
    "\n",
    "### Context\n",
    "An LLM System that has the purpose to provide medical advice should only provide medical advice not advice on financial investments, and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a9d02",
   "metadata": {},
   "source": [
    "## Implementing LLM Safety and Security\n",
    "\n",
    "Let's implement a chain that checks whether the chain stays on topic\n",
    "- The task is to implement a system that consumes a user prompt and returns valid if the prompt is on a healthcare topic, invalid otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9bab4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import  ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(usecwd=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62dd5dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2071a5273a4f7da0e2f491d00a278c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac1757c21a04d31b386c225b82b8299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1b92bdd5cc478f850318069550fceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f852dbedec4952925ae0cdf7a21caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e5339a8e4a49eeab455d74f83f5775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# classification model\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9537bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks the topic\n",
    "def guard_medical_prompt(prompt: str) -> str:\n",
    "    candidate_labels = [\"politics\", \"finance\", \"technology\", \"healthcare\", \"sports\"]\n",
    "    result = classifier(prompt, candidate_labels)\n",
    "    if result[\"labels\"][0] == \"healthcare\":\n",
    "        return \"valid\"\n",
    "    return \"invalid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80611fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'valid'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Test guard_medical_prompt\n",
    "user_prompt1 = \"should I buy stocks of Apple, Google, or Amazon?\"\n",
    "user_prompt2 = \" I have headache\"\n",
    "guard_medical_prompt(user_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4df7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  integrate the guard into a chain\n",
    "def guard_chain(user_input: str):\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a heplful assistant that can answer questions about healthcare.\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "    if guard_medical_prompt(user_input) == \"invalid\":\n",
    "        return \"Sorry, I can only answer questions related to healthcare.\"\n",
    "    chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "    return chain.invoke({\"input\": user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d7133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sorry, I can only answer questions related to healthcare.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test guard chain\n",
    "guard_chain(user_prompt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d9199",
   "metadata": {},
   "source": [
    "### Llama Guard\n",
    "LG is a model created by Meta specifically designed for tasks that involve safeguarding an LLM.\n",
    "- It is a fine-tuned Llama model for content moderation and it works by classifying user prompts or model outputs into classes `safe` or `unsafe`. \n",
    "- For `unsafe` class, there are further hazard subgroups from violent crimes to sexual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da34cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77c37372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_guard_model(user_prompt: str):\n",
    "    model_id = \"meta-llama/Llama-Guard-3-1B\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": user_prompt\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        conversation, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=20,\n",
    "        pad_token_id=0,\n",
    "    )\n",
    "    generated_tokens = output[:, prompt_len:]\n",
    "\n",
    "    res = tokenizer.decode(generated_tokens[0])\n",
    "    if \"unsafe\" in res:\n",
    "        return \"invalid\"\n",
    "    return \"valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f86789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "llama_guard_model(user_prompt=\"How can I perform a scam?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87b6b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploring-top-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
